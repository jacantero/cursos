{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jnfo9Fa-aAFz"
   },
   "source": [
    "**PRÁCTICA 1. FORDWARD & BACKPROPAGATION: EVALUANDO EL PERCEPTRON CON FUNCIONES LÓGICAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMse4l9nFXRw"
   },
   "source": [
    "En esta primera práctica el alumno desarrollará los algoritmos fundamentales del aprendizaje profundo: **fordward** y **backpropagation**. La implementación de ambos algoritmos hará que el estudiante entienda y afiance la información proporcionada en la sesión teórica. Además, gracias a estos ejercicios se entenderán concecptos como la **inicialización de los pesos**, el significado de **época**, cálculo del **error**, **actualización de los pesos**, **bias**, **función de activación** y **datasets lógicos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wM-VJ1eZ4KYW"
   },
   "source": [
    "Rossenblatt definió el Perceptrón como un sistema de aprendizaje supervisado, es decir, un sistema que aprende a partir de datos etiquetados mapeando estas entradas a las diferentes categorias de salida. En su forma más simple el Perceptrón contiene **N** nodos de entrada (uno por cada una de las entradas de nuestra matriz de datos) seguido por una **ÚNICA capa de UN solo nodo** tal y como se puede ver en la siguiente figura:\n",
    "\n",
    "![Perceptron simple](https://drive.google.com/uc?id=1ioUlrmt1AJOwuYZ3gzUfR5ONhtJyThK2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vwu8rFXlY2Yt"
   },
   "source": [
    "Esta simple red conecta nuestro vector de entrada ($x_j$) con el nodo de salida por medio de ciertas conexiones caracterizadas por unos pesos en un instante dado ($w(t)$) junto con el  término bias ($b$) . La predicción de salida por lo tanto se puede obtener como $p_j=w(t)x_j$. El objetivo de la fase de entrenamiento de una red neuronal es optimizar los valores de los pesos $w$ con el objetivo de minimizar el error a la salida entre la predicción ($p_j$) y el *ground truth* ($d_j$). Para ello, cuando una muestra de entrenamiento pase por nuestra red, se obtendrá el error como $e = d_j - p_j$ y se actualizarán los pesos según la ecuación $w_i(t+1) = w_i(t) - \\eta(d_j-p_j)x_{j,i}$ para todas las características $0 \\leq i \\leq n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYZ-tXHbH5ES"
   },
   "source": [
    "**EJERCICIO 1.** En primer lugar  **vamos a programar** en lenguaje **Python** (haciendo uso  de nuevo **exclusivamente** del paquete científico **numpy**) una clase denominada **```Perceptron```** que contenga los métodos necesarios para las fases de entrenamiento y predicción. Concretamente, dicha clase contendra los siguientes métodos:\n",
    "\n",
    "- Método **```__init__```**: Este método será el **constructor de la clase**. A través del mismo instanciaremos un objeto de la clase ```Perceptron```. Dicho método recibirá como parámetros de entrada el número de entradas al perceptrón (**```N```**) y la tasa de aprendizaje (**```eta```**). Si no se proporciona valor de tasa de aprendizaje, por defecto **```eta=0.1```**. Este método debe **inicializar aleatoriamente los pesos** **```W```** siguiendo una **distribución** normal (**Gaussiana**) de media cero y varianza unidad.\n",
    "\n",
    "- Método **```thresholding```**: Este método aplicará un **umbral a las predicciones** para convertirlas en un valor binario. Si $p_j(x)>0$ entonces $\\hat{y} = 1$, en cualquier otro caso $\\hat{y}=0$.\n",
    "\n",
    "- Método **```fit```**: Esta función será la encargada de **\"ajustar\" los datos al modelo**, es decir, se encargará de la **fase de entrenamiento**. Para ello, se recorre un número de épocas dado y para cada época (por defecto **```epochs = 15```**) se calcula el producto de la **entrada por los pesos** (para todas las conexiones de la figura anterior) y se aplica la **función umbral**. Posteriormente se calcula el **error** y se **actualizan los pesos**.\n",
    "\n",
    "- Método **``predict``**: Se encargará de realizar la **predicción de nuevas muestras**. El método devolverá la multiplicación de la entrada por los pesos (obtenidos en la fase de entrenamiento) habiendo aplicado a dicha operación la función umbral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsL7crXM6tDT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "  \n",
    "  def __init__(self, N, eta=0.1):\n",
    "    # Inicializar la matriz de pesos y almacenar la tasa de aprendizaje\n",
    "    #???\n",
    "    #???\n",
    "    \n",
    "  def thresholding(self, x):\n",
    "    # Aplicar a función umbral\n",
    "    #???\n",
    "  \n",
    "  def fit(self, X, y, epochs=15):\n",
    "    # Añadir bias en la última columna de la matriz\n",
    "    X = np.c_[X, np.ones((X.shape[0]))]\n",
    "    # Recorremos el número de épocas pre-establecido\n",
    "    for epoch in np.arange(0, epochs):\n",
    "      # Para cada uno de los puntos del dataset\n",
    "      for (x, target) in zip(X, y): \n",
    "        # Producto de la entrada por los pesos de cada una de las conexiones\n",
    "        # (producto matricial, i.e. dot en numpy) y aplicar la función umbral\n",
    "        #???\n",
    "        # Actualizar pesos en caso de que la predicción y el ground truth sean distintos\n",
    "        if p != target:\n",
    "          # Calculo del error\n",
    "          #???\n",
    "          # Actualización de pesos\n",
    "          #???\n",
    "          \n",
    "  def predict(self, X, addBias=True):\n",
    "    # Aseguramos que la entrada es una matriz\n",
    "    X = np.atleast_2d(X)\n",
    "    # Añadimos el bias a las muestras de test si es necesario\n",
    "    if addBias:\n",
    "      X = np.c_[X, np.ones((X.shape[0]))]\n",
    "    # Devolevmos la etiqueta con la prediccón\n",
    "    #???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhHkMuQP8Og0"
   },
   "source": [
    "**EJERCICIO 2.** Despues de programar la clase ```Perceptron``` vamos a  **entrenarlo**. Para ello, en primer lugar debemos **construir el set de datos OR**. Crea un primer numpy array **```X```** que contenga una **lista** en la que cada entrada sea una **pareja de bits [$x_1$, $x_2$]** (ver transparencia proyectada) y un segundo numpy array **```y```** que contenga una lista con la **salida del operador lógico** en cuestión para cada pareja de entradas. Posteriormente, se debe instanciar un objeto de la clase ``Perceptron`` con el valor de **```alpha```** por defecto y efectuar el proceso de entrenamiento durante **```epochs=20```**. Por último podemos **evaluar nuestro perceptrón** en cada pareja de puntos del numpy array de datos de entrada **```X```** para comprobar si es capaz de modelar el comportamiento del dataset lógico OR. Comente brevemente los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly8MlwmpfZNV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construimos el dataset OR\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Definimos nuestro perceptrón y lo entrenamos\n",
    "print(\"[INFO]: Training perceptron with the OR dataset...\")\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Evaluemos nuestro preceptrón\n",
    "print(\"[INFO]: Testing perceptron with the OR dataset...\")\n",
    "for (x, target) in zip(X, y):\n",
    "  #???\n",
    "  print(\"[INFO]: data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNdgDOS4GvsP"
   },
   "source": [
    "A continuación **replica el ejercicio** anterior para las **funciones lógicas AND y XOR** en dos celdas distintas. Comente de nuevo los resultados obtenidos. ¿A que crees que se debe lo que ocurre con el set de datos XOR? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "255ZFlUzggh_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construimos el dataset AND\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Definimos nuestro perceptrón y lo entrenamos\n",
    "print(\"[INFO]: Training perceptron with the AND dataset...\")\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Evaluemos nuestro preceptrón\n",
    "print(\"[INFO]: Testing perceptron with the AND dataset...\")\n",
    "for (x, target) in zip(X, y):\n",
    "  #???\n",
    "  print(\"[INFO]: data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I52VMQRgq5o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construimos el dataset XOR\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Definimos nuestro perceptrón y lo entrenamos\n",
    "print(\"[INFO]: Training perceptron with the XOR dataset...\")\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Evaluemos nuestro preceptrón\n",
    "print(\"[INFO]: Testing perceptron with the XOR dataset...\")\n",
    "for (x, target) in zip(X, y):\n",
    "  #???\n",
    "  print(\"[INFO]: data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAWTaMStt3-D"
   },
   "source": [
    "No importa cuantas veces se lleve a cabo el proceso de entrenamiento variando la tasa de aprendizaje o inicializando de distintas formas los pesos de las capas, **el Perceptrón de una única capa NUNCA sera capaz de aprender las no linealidades** del conjunto de datos XOR. Es por ello que en el siguiente ejercicio vamos a implementar lo que podriamos considerar nuestra primera red neuronal del curso, el **perceptrón multicapa**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HySE6jpowoKr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myyix3fODAvm"
   },
   "source": [
    "El perceptron multicapa (***multilayer perceptron*** en inglés) esta compuesto por al menos una capa oculta o ***hidden layer***. Tal y como hemos visto en el apartado teórico, esta capa oculta está **compuesta por una serie de neuronas** cuya función es **aplicar una no linealidad al producto de los pesos por las entradas a dicha capa** (ya sean las entradas a la red o las salidas de una capa oculta anterior). \n",
    "\n",
    "![Perceptron_multicapa](https://drive.google.com/uc?id=1gG4098wxVLS54RW89VW2cyAV7ysOcoWd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-oEuSOUEmmJ"
   },
   "source": [
    "\n",
    "Es en este tipo de redes en el que al menos existe una capa oculta, cuando entra en juego el algoritmo de propagación hacia detrás o ***backpropagation***. Dicho algoritmo es sin duda el  más importante en la historia de las redes neuronales. Sería impensable entrenar redes neuronales de una profundidad como la actual sin esta técnica.\n",
    "\n",
    "El algoritmo de backpropagation se divide en **dos fases**:\n",
    "\n",
    "1.   El paso hacia adelante o ***forward propagation*** donde nuestras entradas atraviesan la red neuronal y obtenemos a la salida una predicción para las mismas. En la arquitectura de la figura anterior, las predicciones se obtienen como $p(x) = f(a\\cdot W^1+b_1)$, siendo $a=f(x \\cdot W^0+b_0)$.\n",
    "2.   El paso hacia detrás o ***backward propagation*** donde se calcula el gradiente de la función de pérdidas a la salida de la última capa de la red. Dicho gradiente se utiliza posteriormente para actualizar los pesos de nuestra red de manera recursiva.\n",
    "\n",
    "Para aplicar el algoritmo de retropropagación, es necesario que la función de activación empleada sea diferenciable para poder calcular las derivadas parciales del error con respecto a un peso dado $w_{i,j}$ siguiendo la regla de la cadena:\n",
    "\n",
    "$\\dfrac{\\partial E}{dw_i^j} = \\dfrac{\\partial E}{\\partial a_j}\\dfrac{\\partial a_j}{\\partial z_j}\\dfrac{\\partial z_j}{\\partial w_i^j}$, donde $E$ hace referencia al error o pérdidas a la salida de la red, $a_j$ es la salida de la neurona $j$ (tras la función de activación), y $z_j$ es la salida de la neurona $j$ antes de aplicar la función de activación (i.e. producto entradas (activaciones) por pesos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72CRNOyX1Lkf"
   },
   "source": [
    "**EJERCICIO 3.** En este ejercicio **vamos a desarrollar** en lenguaje **Python** (haciendo uso  de nuevo **exclusivamente** del paquete científico **numpy**) los algoritmos de forward y backpropagation. Para ello vamos a desarrollar **una clase denominada ```NeuralNetwork```** que contendrá un serie de métodos que analizamos a continuación:\n",
    "\n",
    "- Método **```__init__```**: Este método será el **constructor de la clase**. A través del mismo instanciaremos un objeto de la clase ```NeuralNetwork```. Dicho método recibirá como parámetros de entrada una lista (variable **```layers```**)  con el número de neuronas por capa que conformarán la arquitectura de red y la tasa de aprendizaje (**```eta```**). Si no se proporciona valor de tasa de aprendizaje, por defecto **```eta=0.1```**. Este método debe **inicializar aleatoriamente los pesos** **```W```** que conectan **cada una** de las capas de la red siguiendo una **distribución** normal (**Gaussiana**) de media cero y varianza unidad.\n",
    "​\n",
    "- Método **```__repr__```**: Este método muestra por pantalla la arquitectura de red implementada.\n",
    "\n",
    "- Método **```sigmoid```**: Contendrá la ecuación de la función de activación sigmoide.\n",
    "\n",
    "- Método **```sigmoid_deriv```**: Contendrá la derivada de la función de activación. La emplearemos en el cálculo del error hacia detrás en el algortimo de *backpropagation*. \n",
    "\n",
    "- Método **```fit```**: Esta función será la encargada de **\"ajustar\" los datos al modelo**, es decir, se encargará de la **fase de entrenamiento**. Para ello, se recorre un número de épocas dado y para cada época (por defecto **```epochs = 1000```**) se llama a la función **```fit_partial```** para cada uno de los pares de datos de entrada. Posteriormente, se calcula el error (pérdidas) y se almacenan para poder graficarlas posteriormente.\n",
    "\n",
    "- Método **```fit_partial```**: Es el encargado del proceso de entrenamiento como tal. Dicho método se puede dividir en tres fases:\n",
    "\n",
    ">1.   Propagación hacia delante (**Fordward propagation**): Producto de la **entrada por los pesos** y **función de activación para cada una de las capas** de la arquitectura (almacenar en una lista). \n",
    "2.   Propagación del error hacia detrás (**Backpropagation**): Cálculo del **error a la salida**, y obtención del error **en cada capa** aplicando la **\"regla de la cadena\"**. Recuerda que debes recorrer las capas de atrás hacia delante e ir calculando la delta (error) asociada a la capa en cuestión como:\n",
    "\n",
    ">>>>>>>>$d^{l-1} = (d^l*(W^{l-1})^T)\\cdot f'(a^{l-1})$ \n",
    "\n",
    ">3. Actualización de los pesos (**Weights update**): Actualizar los pesos **de cada capa** según la ecuación para actualizar los pesos vista en la clase teórica y en la introducción a la práctica.\n",
    "\n",
    "- Método **``calculate_loss``**: Cálculo del error cuadrático medio a partir de las etiquetas y las predicciones. \n",
    "\n",
    "- Método **``predict``**: Se encargará de realizar la **predicción de nuevas muestras**. El método devolverá la salida de la red. Pada cada capa realizará la multiplicación de la entrada por los pesos obtenidos en la fase de entrenamiento y aplicará la función de activación. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYT7D9I0Di_S"
   },
   "outputs": [],
   "source": [
    "# Importamos la única librería con la que desarrollaremos nuestra primera NN\n",
    "import numpy as np\n",
    "# Una librería más solo para propósitos de visualización\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork:\n",
    "  def __init__(self, layers, eta=0.1):\n",
    "    # Inicialicemos una lista de pesos y almacenemos la arquitectura de red y lr\n",
    "    self.W = []\n",
    "    self.layers = layers # Lista de enteros, ejem [2,2,1] significa que tenemos dos entradas, una capa oculta de dos nodos y una neurona de salida. \n",
    "    self.eta = eta # Tasa de aprendizaje\n",
    "    # Inicializando pesos de las capas\n",
    "    for i in np.arange(0, len(layers) - 2): \n",
    "      #??? # Matriz M x N (salida capa i/entrada capa i+1) de pesos siguiendo distr. gaussiana (+1 por el bias) #(X)\n",
    "      self.W.append(w / np.sqrt(layers[i])) # Almacenamos pesos escalados\n",
    "    #??? # Inicialización última capa #(X)\n",
    "    self.W.append(w / np.sqrt(layers[-2]))\n",
    "  \n",
    "  def __repr__(self):\n",
    "    # Devuelve un string con la arquitectura de la red\n",
    "    return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    "  \n",
    "  def sigmoid(self, x):\n",
    "    #???\n",
    "  \n",
    "  def sigmoid_deriv(self, x):\n",
    "    # La derivada de la función sigmoide f(x) es f'(x) = f(x) * (1 - f(x))\n",
    "    # Supondremos que aquí ya entra f(x), es decir, que nos entra la salida de la sigmoide\n",
    "    #???\n",
    "  \n",
    "  def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "    # Bias trick: Concatenamos los bias para que sean parámetros entrenables de la red\n",
    "    X = np.c_[X, np.ones((X.shape[0]))]\n",
    "    # Lista para almacenar pérdidas para hacer un plot Loss vs epochs\n",
    "    my_losses = []\n",
    "    # Recorremos épocas\n",
    "    for ep in np.arange(0, epochs):\n",
    "      # Recorremos datos de entrada y entrenamos red\n",
    "      for (x, target) in zip(X, y):\n",
    "        self.fit_partial(x, target)\n",
    "      # Calculamos pérdidas de todos los datos de entrenamiento\n",
    "      loss = self.calculate_loss(X,y)\n",
    "      my_losses.append(loss)\n",
    "      # Muestro una de cada displayUpdate muestras\n",
    "      if ep == 0 or (ep + 1) % displayUpdate == 0:  \n",
    "        print(\"[INFO]: epoch={}, loss={:.7f}\".format(ep + 1, loss))\n",
    "    # Visualización de la curva Loss vs Epochs\n",
    "    plt.plot(np.arange(0, epochs), my_losses, 'r')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.title('Pérdidas en la fase de entrenamiento en XOR')\n",
    "    plt.show()\n",
    "        \n",
    "  def fit_partial(self, x, y):\n",
    "    # Lista de activaciones para cada capa conforme el dato pasa por la red\n",
    "    # La primera activación es un caso especial, el vector de características en si mismo\n",
    "    A = [np.atleast_2d(x)]\n",
    "    \n",
    "    # 1. PROPAGACIÓN HACIA DELANTE (FEEDFORWARD)\n",
    "    for layer in np.arange(0, len(self.W)):\n",
    "      #??? # Multiplicación de activación por pesos actual (X)\n",
    "      #??? # Función de activación (X)\n",
    "      #??? # Añadimos la activación en cuestión a nuestra lista de activaciones (X)\n",
    "      \n",
    "    # 2. RETROPROPAGACIÓN (BACKPROPAGATION)\n",
    "    #??? # Cáculo del error total(X)\n",
    "    #??? #dE/do\n",
    "    # Recorremos las capas para ir calculando las derivadas parciales\n",
    "    # La última capa ya la hemos tenido en cuenta en el cáculo de la primera delta\n",
    "    for layer in np.arange(len(A) - 2, 0, -1):\n",
    "      # La nueva delta es la anterior multiplicada matricialmente por los pesos de la capa actual (traspuestos)\n",
    "      # seguido del producto entre la delta y la derivada de la función de activación\n",
    "      #???\n",
    "      #???\n",
    "      D.append(delta)\n",
    "    # Invertimos la matriz D para tener ordenadas nuestras deltas según la red\n",
    "    D = D[::-1]\n",
    "    \n",
    "    # 3. FASE DE ACTUALIZACIÓN DE PESOS (Aquí es dónde el aprendizaje se lleva a cabo)\n",
    "    # Los nuevos pesos serán los antiguos pesos menos (dirección gradiente) el producto\n",
    "    # de las activaciones de la capa en cuestión por el producto matricial de las deltas de dicha capa\n",
    "    for layer in np.arange(0, len(self.W)):\n",
    "      #???\n",
    "     \n",
    "  def predict(self, X, addBias=True):\n",
    "    # Inicializamos la salida de la predicción con los valores de entrada\n",
    "    p = np.atleast_2d(X)\n",
    "    # Comprobar si hay que añadir el termino del bias\n",
    "    if addBias:\n",
    "      p = np.c_[p, np.ones((p.shape[0]))]\n",
    "    # Vamos recorriendo todas las capas y computamos la activación en cuestión\n",
    "    for layers in np.arange(0, len(self.W)):\n",
    "      #???\n",
    "    return p\n",
    "  \n",
    "  def calculate_loss(self, X, targets):\n",
    "    # Predecimos las entradas y calculamos pérdidas\n",
    "    targets = np.atleast_2d(targets)\n",
    "    predictions = self.predict(X, addBias=False)\n",
    "    #???\n",
    "    return loss\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUiotzfh708h"
   },
   "source": [
    "**EJERCICIO 4.** Despues de programar la clase ```NeuralNetwork``` vamos a  **entrenar** nuestra primera red neuronal. Para ello, en primer lugar debemos **construir el set de datos XOR** de la misma forma que en el EJERCICIO 2. Posteriormente, se debe instanciar un objeto de la clase ``NeuralNetwork`` con el valor de **```eta = 0.5```** y efectuar el proceso de entrenamiento durante **```epochs=5000```**. \n",
    "Por último podemos **evaluar nuestro perceptrón** en cada pareja de puntos del numpy array de datos de entrada **```X```** para comprobar si es capaz de modelar el comportamiento del dataset lógico XOR. Comente brevemente los resultados obtenidos. \n",
    "\n",
    "- Lleva a cabo diversas realizaciones del proceso entrenamiento-aprendizaje. ¿Se observan los mismos resultados de una ejecución a otra?¿ A que crees que es debido? \n",
    "\n",
    "- ¿Cual es el menor número de épocas en el cual sigue convergiendo la red a la solución óptima? \n",
    "\n",
    "- ¿Que sucede si dejamos fijo el número de épocas a  **```epochs=5000```** y empleamos el valor de **```eta```** por defecto?¿Por que sucede esto? \n",
    "\n",
    "- Experimente con distintas arquitecturas de red, ¿Que ocurre si entrena una red [2,1]?¿Por qué sucede esto? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HXbo2eaEb1G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construimos el dataset XOR\n",
    "#???\n",
    "#???\n",
    "\n",
    "# Definición de arquitectura haciendo uso de la clase anterior\n",
    "#???\n",
    "\n",
    "# Entrenamiento de la misma\n",
    "#???\n",
    "\n",
    "# Fase de predicción\n",
    "for x, target in zip (X,y):\n",
    "  #???\n",
    "  label = 1 if pred > 0.5 else 0\n",
    "  print(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(x, target[0], pred, label))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P1. Fordward y Backpropagation:  Evaluando el perceptron con funciones logicas_JUN19.ipynb",
   "provenance": [
    {
     "file_id": "1sAkDgaxoY-UklExWnYd5h5eU2UW7a4yW",
     "timestamp": 1542623481066
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
