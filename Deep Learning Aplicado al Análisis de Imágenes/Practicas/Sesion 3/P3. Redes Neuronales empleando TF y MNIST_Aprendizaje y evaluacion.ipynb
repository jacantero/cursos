{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P3. Redes Neuronales empleando TF y MNIST_Aprendizaje y evaluacion.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PdTU7ecHlo4P","colab_type":"text"},"source":["**PRÁCTICA 3. REDES NEURONALES EMPLEANDO TF Y MNIST: APRENDIZAJE Y EVALUACIÓN**"]},{"cell_type":"markdown","metadata":{"id":"plPMuRcwbD_p","colab_type":"text"},"source":["Después de familiarizarnos con la librería TensorFlow es el momento de implementar la segunda red neuronal del curso. El objetivo de dicha red va a ser el de **identificar digitos del 0 al 9 escritos de forma manual**. El set de datos **MNIST** es un conjunto de  70000 imágenes de $28 \\times 28$ pixels que contienen números manuscritos junto con la etiqueta solución del número codificado (i.e. nuestro ground truth). \n","\n","![mnist](https://drive.google.com/uc?id=1uvFNAFTVb58xPoSt_3F_9DLs47ZIyPFm)\n","\n","Cabe destacar que, por convención, MNIST dispone de una división específica en conjuntos de entrenamiento, validación y test. Este dataset es el \"Hola Mundo\" del aprendizaje profundo y es de gran utilidad para validar nuevos métodos propuestos ya que hace de *benchmark* permitiendo establecer comparativas justas. Debido a su gran popularidad las principales librerias destinadas al aprendizaje profundo permiten cargarlo directamente en nuestro código. Así que sin más dilación vamos a cargar los datos y a visualizarlos:"]},{"cell_type":"code","metadata":{"id":"tm2jKyXgZ1Rs","colab_type":"code","colab":{}},"source":["# Imports necesarios\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# Importamos el dataset MNIST y cargamos los datos\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n","\n","# Comprobar el tamaño del dataset (presta atención a como se accede a cada partición de los datos)\n","print(\"El conjunto de entrenamiento tiene dimensiones: \", mnist.train.images.shape)\n","print(\"El conjunto de validación tiene dimensiones: \",mnist.validation.images.shape)\n","print(\"El conjunto de test tiene dimensiones: \",mnist.test.images.shape)\n","\n","# Método para visualizar los datos de entrenamiento\n","def display_digit(num):\n","  # Seleccionar la imagen num de mnist.train.images y hacer un reshape al tamaño de la imagen\n","  #???\n","  # Seleccionar el target num de mnist.train.labels (Recuerda que esta en one-hot encoding, conviertelo a decimal con el método np.argmax)\n","  #???\n","  # Mostrar\n","  plt.title('Example: %d  Label: %d' % (num, label))\n","  plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n","  plt.show()\n","\n","# Mostramos algunos ejemplos\n","display_digit(np.random.randint(0, mnist.train.images.shape[0]))\n","display_digit(np.random.randint(0, mnist.train.images.shape[0]))\n","display_digit(np.random.randint(0, mnist.train.images.shape[0]))\n","\n","# ¡¡¡ Ejecuta el código varias veces y comprueba la variabilidad existente en los datos !!!"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwKVmorKmdsP","colab_type":"text"},"source":["**EJERCICIO 1.** Una vez visualizadas ciertas muestras de nuestro conjunto de datos,  vamos a entrenar un **perceptrón simple** (como el de la primera sesión) con 10 neuronas en la capa de salida (una por dígito). Recordad que en el perceptrón simple las entradas se ponderan por ciertos pesos y se suman en cada una de las neuronas de salida. Posteriormente emplearemos la función **Softmax** que hemos explicado en la sesión teórica, calculando las predicciones como $\\hat{Y}=softmax(X∗W+B)$ y minimizando la función de entropía cruzada o **cross-entropy** siguiendo la formula: \n","\n",">>>>>>>>$Coste = - \\displaystyle\\sum_j y_j log(p_j)$\n","\n","donde $y_j$ es el *ground truth* para la clase $j$ y $p_j$ el valor de probabilidad asignado a dicha clase a la salida."]},{"cell_type":"code","metadata":{"id":"yO17EjSLoEpv","colab_type":"code","colab":{}},"source":["# Hiper-parámetros de nuestra red\n","lr = 0.005\n","n_epochs = 100\n","batch_size = 128\n","\n","# Creamos placeholders (X e Y_true) para ir almacenando los datos de entrada y los labels\n","# Cuando definas los placeholders, indica tipo de datos float 32 y las dimensiones adecuadas\n","# Ten en cuenta que la forma de un tensor viene definida por (batch_size, rows, cols, channels) \n","# y el batch_size es un parámetro variable\n","# ??? # Imágenes del mnist: 28*28 = 784\n","# ??? # Número indicando la clase 0-9 => 10 clases\n","\n","# Creamos e inicializamos con ceros las variables W y b (indicando las dimensiones)\n","# ???\n","# ???\n","\n","# Calculamos las predicciones (emplea tf.nn.softmax). Alamcenalas en una variable llamada Y_pred\n","# ???\n","\n","# Definimos función de pérdidas cross_entropy (ver tf.reduce_sum). Tened en cuenta que las pérdidas se calculan por batch (tf.reduce_mean)\n","# ???\n","\n","# Optimizador SGD\n","# ???\n","\n","# % de predicciones correctas en un determinado batch (i.e. accuracy)\n","is_correct = tf.equal(tf.argmax(Y_pred,1), tf.argmax(Y_true,1))\n","accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n","\n","# Inicializamos variables\n","# ???\n","\n","# Abrimos la sesión\n","with tf.Session() as sess:\n","  # Ejecutamos inicialización de variables\n","  # ???\n","  # Entrenamiento de nuestra red\n","  acc_epoch_tr = []\n","  acc_epoch_val = []\n","  loss_epoch_tr = []\n","  loss_epoch_val = []\n","  for epoch in range(n_epochs):\n","    avg_acc = 0.\n","    avg_loss = 0.\n","    #Calcular número de batches\n","    # ???\n","    for i in range(steps):\n","      # Pedir un nuevo batch del set de datos (emplear función next_batch)\n","      # ???\n","      # Entrenamos pasando los datos del batch\n","      # ???\n","      # Calculamos accuracy y cross_entropy del batch\n","      # ???\n","      avg_acc += a / steps # Accuracy medio de los diferentes batches\n","      avg_loss += l / steps # Pérdidas medias de los diferentes batches\n","    # Almacenamos el accuracy y las losses medios en las listas correspondientes para cada época (acc_epoch_tr, loss_epoch_tr)\n","    # ???\n","    # ???\n","    # Calculamos accuracy y losses en validation\n","    # ???\n","    # Almacenamos en las listas correspondientes para cada época (acc_epoch_val, loss_epoch_val)\n","    # ???\n","    # ???\n","    # Sacamos información por pantalla\n","    print(\"[INFO]: Época {} ---> Acc_train = {} - Loss_train = {} - Acc_val = {} - Loss_val = {}\".format(epoch, avg_acc, avg_loss, a_val, l_val))\n","  \n","  # Cálculo de accuracy y losses en el conjunto de test\n","  # ???\n","  print(\"[INFO]: Accuracy en test = {} - Losses en test = {}\".format(a_test, l_test)) #(X)\n","  \n","  # Gráficar losses por época\n","  # ??? # Plot pérdidas en training\n","  # ??? # Plot pérdidas en validation\n","  # ??? # Leyenda\n","  # ??? # Título\n","  # ??? # Label eje x\n","  # ??? # Label eje y\n","  \n","  #¿Que se puede observar de las gráficas?¿Según hemos visto en la teoría, se aprecia overfitting?"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7CTQkzMwWuR","colab_type":"text"},"source":["**EJERCICIO 2.** Si todo ha ido bien, vuestro perceptron simple con TensorFlow será capaz de reconocer con una precisión en torno al 92% dígitos del 0 al 9 escritos a mano! A continuación vamos a **incluir una primera hidden layer de 200 neuronas** al perceptrón simple que hemos desarrollado en el ejercicio anterior para **descubrir el efecto de dar profundidad a nuestra red** creando ya un *multilayer perceptron*. Para ello habrá que definir las variables ```(W1, B1)``` y ```(W2, B2)``` en TensorFlow y re-calcular  las predicciones como $\\hat{Y} = softmax(Y1*W2 + B2)$ siendo $Y1 = sigmoid(X*W1+ B1)$.\n","\n","Hay que tener en cuenta que conforme le damos profundidad a la red es importante inicializar los pesos (```W1``` y ```W2``` en nuestro caso) con **valores aleatorios siguiendo una distribución gaussiana**. De otra forma, el optimizador puede quedarse atascado en la posición inicial y no ser capaz de moverse en la dirección del mínimo de la función de pérdidas. Por ello, utilizaremos la función de TensorFlow  **``` tf.truncated_normal```**, tal y como se indica en la documentación de TF.\n","\n","Además vamos a aprovechar este ejercicio para convertir nuestro código dejado caer en un script a una función cuya cabecera será la siguiente:\n","\n","```\n","def train_shallow_net(learning_rate, batch_size, num_epochs)\n","```\n","\n","**Nota**: Copiad y pegad el código anterior en una nueva celda y convertidlo en función. Posteriormente añadid la segunda capa oculta\n","\n","1. Tras la modificación, se debe ejecutar la función ```train_shallow_net``` con sus parámetros de entrada tomando el valor del apartado anterior. ¿Qué sucede?\n","\n","2. Ahora, lanza de nuevo el entrenamiento de nuestra shallow net pero con un valor de **```n_epochs = 1000```**. ¿Que sucede ahora?¿Se aprecia la influencia de darle profundidad a la red?¿Existe overfitting? \n","\n","**Nota**: La última ejecución tardará unos 20 minutos, por lo que puedes aprovechar para leer detenidamente el siguiente apartado."]},{"cell_type":"code","metadata":{"id":"F1WZgNff6oYg","colab_type":"code","colab":{}},"source":["def train_shallow_net(learning_rate, batch_size, num_epochs):\n","  # Creamos placeholders para ir almacenando los datos de entrada y los labels\n","  # ???\n","  # ???\n","\n","  # Creamos e inicializamos las variables W1 y W2 con valores aleatorios que sigan una distribución normal\n","  # Podemos seguir inicializando los biases (B1 y B2) a 0 o un valor positivo muy pequeño\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","\n","  # Calculamos las predicciones siguiendo las ecucaiones del enunciado\n","  # ???\n","  # ???\n","  \n","  # Completamos la función con el código del ejercicio anterior\n","  # ........\n","  \n","# Lanzamos entrenamiento\n","lr = # ???\n","b_size = # ???\n","n_epochs = # ???\n","train_shallow_net(lr, b_size, n_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jjBpQkHKzNGw","colab_type":"text"},"source":["**EJERCICIO 3.** A continuación vamos a relizar **una serie de ejecuciones** empleando la función desarrollada en el apartado anterior con el objetivo de **analizar la influencia** que tienen los parámetros **```learning_rate```** y **```batch_size```**.\n","\n","- En primer lugar vamos a fijar los valos valores de **```batch_size = 128```** y **```num_epochs=15```** y vamos a realizar tres ejecuciones de nuestra red (en tres celdas distintas) con valores de **```learning_rate = 4```**, **```learning_rate = 0.1```** y **```learning_rate = 0.0001```**, respectivamente.\n","\n","\n","1.   ¿Que se puede observar? Relacionad las gráficas de pérdidas obtenidas con la gráfica que os proporcionamos en el apartado teórico de la explicación de la tasa de aprendizaje. "]},{"cell_type":"code","metadata":{"id":"pVGYgzQ_KUXT","colab_type":"code","colab":{}},"source":["# Lanzamos la primera ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xz2cd9sqMRMX","colab_type":"code","colab":{}},"source":["# Lanzamos la segunda ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lktG_qbPMVBU","colab_type":"code","colab":{}},"source":["# Lanzamos la tercera ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqVpYrnMkZIb","colab_type":"text"},"source":["- En segundo lugar, **vamos a fijar la mejor tasa de aprendizaje de las obtenidas anteriormente** (obviando los valores de learning rate que propicien *overfitting*), es decir,  **```learning_rate = 0.01```**. El número de épocas lo mantenemos en **```num_epochs=15```** (para observar los resultados en un tiempo permisible) y vamos a realizar **cuatro ejecuciones de nuestra red** (en cuatro celdas distintas) con valores de **```batch_size = 32```**, **```batch_size = 256```**, **```batch_size = 2048```** y  **```batch_size = 8192```** respectivamente.\n"," \n","\n","1.   ¿Que se puede observar atendiento al apartado temporal?¿Que influencia tiene el batch size en el tiempo de ejecución?¿Por que sucede esto? \n","\n","2.   ¿Que ocurre para un **```batch_size = 32```** desde el punto de vista de la clasificación? ¿Que parámetro se podría variar para evitar este suceso negativo?\n","\n","3. ¿Que ocurre para un **```batch_size = 8192```** desde el punto de vista de la clasificación?¿Que parámetro se podría variar para mejorar el aprendizaje? "]},{"cell_type":"code","metadata":{"id":"JhbfyBP7bmgP","colab_type":"code","colab":{}},"source":["# Lanzamos la primera ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuV6deLHiSml","colab_type":"code","colab":{}},"source":["# Lanzamos la segunda ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0XnYZUHiXAQ","colab_type":"code","colab":{}},"source":["# Lanzamos la tercera ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dho0DJ_3ikGl","colab_type":"code","colab":{}},"source":["# Lanzamos la cuearta ejecución\n","# ???\n","# ???\n","# ???\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XNnKOv6CAR5G","colab_type":"text"},"source":["**EJERCICIO 4.** Esta primera red que habeís desarrollado es una **shallow-net**. A partir de dos capas ocultas en adelante es cuando podemos hablar de redes neuronales profundas. Ahora vamos a realizar una nueva modificación en nuestra función (debemos definirla en una nueva celda con las modificaciones). Concretamente, **vamos a darle más profundidad a nuestra red** añadiendo tres capas ocultas más, a parte de las que ya tenemos, es decir, nuestra **deep net** estará compuesta por la entrada, **cuatro capas ocultas con 200, 100, 60 y 30 neuronas, respectivamente** y la **capa de salida con las 10 neuronas**. Además, vamos a hacer dos modificaciones más y vamos a extraer como parámetros de entrada a la función la capa de activación de las **hidden layers** y el optimizador. Con estos cambios,  cabecera el nuevo método tendrá la siguiente cabecera:\n","\n","```\n","def train_deep_net(learning_rate, batch_size, num_epochs, act_function, optimizer)\n","```\n","\n","**Nota**: Copiad y pegad el código de la función anterior en una nueva celda, añadid tres capas ocultas extras y modificar la función de activación y el optimizador según el parámetro de entrada a la función.\n","\n","- Tras haber realizado las modificaciones oportunas y disponiendo de la función ```train_deep_net```, llama a dicho método pasándole como parámetros de entrada los siguientes valores: **```learning_rate = 0.005```**, **```batch_size = 128```**, **```num_epochs = 50```**, **```act_function = tf.nn.relu```** y **```optimizer = tf.train.AdamOptimizer```**. ¿Que se puede observar de las curvas de pérdidas de training y validation?¿A que es debido dicho efecto?\n"]},{"cell_type":"code","metadata":{"id":"1f2q9gkgDLd9","colab_type":"code","colab":{}},"source":["def train_deep_net(learning_rate, batch_size, num_epochs, act_function, optimizer):\n","  # Creamos placeholders para ir almacenando los datos de entrada y los labels\n","  # ???\n","  # ???\n","\n","  # Creamos e inicializamos las variables W1, W2, W4, etc. con valores aleatorios que sigan una distribución normal\n","  # Podemos seguir inicializando los biases (B1, B2, B3, etc.) a 0 o un valor positivo muy pequeño\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","\n","  # Calculamos las predicciones: ¡¡¡ utilizar la variable act_function para hacerlo genérico!!!\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # Obten el producto (+ bias) de la última capa oculta y almacenalo en una variable llamada Y_logits\n","  # Será necesario para utilizar tf.nn.softmax_cross_entropy_with_logits_v2 que evita las inestabilidades numéricas de log(0)\n","  # ???\n","  # Obtén Y_pred aplicando softmax\n","  # ???\n","\n","  # Definimos función de pérdidas empleando tf.nn.softmax_cross_entropy_with_logits_v2 que evita las inestabilidades numéricas de log(0)\n","  # ???\n","\n","\n","  # Optimizador SGD: ¡¡¡ utilizar la variable optimizer para hacerlo genérico !!!\n","  # ???\n","\n","  # Completamos la función con el código restante del EJERCICIO 2\n","  # ........\n","    \n","# Lanzamos entrenamiento\n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sr_0aZX9MIfm","colab_type":"text"},"source":["**EJERCICIO 5.** Con el objetivo de subsanar el problema anterior, vamos a realizar la **última modificación de nuestra función ```train_deep_net```**. Concretamente, **vamos a añadir una capa de dropout**, despues de cada una de las fully-connected layers (tras aplicar la función de activación). Para ello, será necesario **declarar un nuevo placeholder** que albergue el porcentaje de neuronas (**$pkeep$**) de una capa que se desean mantener vivas. Como hemos visto en la sesión teórica, una capa de dropout eliminará aleatoriamente el $(1-pkeep)*100 \\%$ de neuronas de la capa oculta anterior. La nueva cabecera de nuestra función quedará de la siguiente manera:\n","```\n","def train_deep_net(learning_rate, batch_size, num_epochs, act_function, optimizer, pkeep_tr)\n","```\n","**Nota**: Recuerda que tanto para validación test el valor de $pkeep=1$, es decir, se deben conservar todas las neuronas de la arquitectura de red.\n","\n","- Tras haber realizado las modificaciones oportunas y disponiendo de la nueva versión de la función ```train_deep_net```,  vuelve a llamar a la función con los mismos parámetros de entrada del apartado anterior y con un valor de **```pkeep_tr= 0.75```**.  Posteriormente, en una nueva celda de código, realiza otra ejecución pero esta vez con un valor de **```pkeep_tr= 0.5```**. Comenta los resultados obtenidos. "]},{"cell_type":"code","metadata":{"id":"9k04PAXUG_dU","colab_type":"code","colab":{}},"source":["def train_deep_net(learning_rate, batch_size, num_epochs, act_function, optimizer, pkeep_tr):\n","  # Creamos placeholders para ir almacenando los datos de entrada y los labels\n","  # ???\n","  # ???\n","  # Placeholder para dropout (pkeep)\n","  # ???\n","  \n","  # Creamos e inicializamos las variables W y b de forma idéntica al EJERCICIO 4\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","\n","  # Calculamos las predicciones añadiendo una capa de dropout (tf.nn.dropout)\n","  # Las capas de dropout se situan despues del cálculo de cada activación (exceptuando la activación final)\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  # ???\n","  \n","  # Completamos la función con el código restante del EJERCICIO 5 pero modificando cada sess.run(train)\n","  # A parte de los placeholders de datos X e Y_true debemos pasarle el placeholder de dropout\n","  # Es MUY IMPORTANTE recordar que para validation y test pkeep = 1 mientras que para entrenamiento pkeep = pkeep_tr\n","  # ........\n","\n","# Lanzamos la primera ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJySFAoqYj6d","colab_type":"code","colab":{}},"source":["# Lanzamos la segunda ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xeAJ74LoManY","colab_type":"text"},"source":["**EJERCICIO 6.** Para finalizar la sesión práctica, vamos a lanzar diversas ejecuciones de nuestra red neuronal profunda para **analizar el comportamiento** del proceso de entrenamiento cuando se emplean **diferentes funciones de activación y optimizadores**.\n","\n","- En primer lugar, realiza diversas ejecuciones de la red anterior (**```pkeep_tr= 0.5```**) llamando a  ```train_deep_net```  con las siguientes funciones de activación: **```act_function = tf.nn.tanh```**, **```act_function = tf.nn.sigmoid```** , **```act_function = tf.nn.leaky_relu```** , **```act_function = tf.nn.leaky_elu```** y **```act_function = tf.nn.leaky_selu```**  . ¿Que observas en las diferentes ejecuciones?¿Que función de activación es la que mejor funciona?¿Es muy alto el impacto de la función de activación en la precisión del modelo para esta aplicación? "]},{"cell_type":"code","metadata":{"id":"ezZD5UKqPI51","colab_type":"code","colab":{}},"source":["# Lanzamos la primera ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lszDv9E9OyiH","colab_type":"code","colab":{}},"source":["# Lanzamos la segunda ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_WfSy9tPJcL","colab_type":"code","colab":{}},"source":["# Lanzamos la tercera ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqKVJZ3YPJ0G","colab_type":"code","colab":{}},"source":["# Lanzamos la cuarta ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhxRxoLtPLGj","colab_type":"code","colab":{}},"source":["# Lanzamos la quinta ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dzs_vb78g2wA","colab_type":"text"},"source":["- Para finalizar, realiza diversas ejecuciones de la red anterior (**```pkeep_tr= 0.5```**) empleando **```act_function = tf.nn.sigmoid```** con los siguientes optimizadores: **```tf.contrib.opt.NadamOptimizer```** y **```tf.train.RMSPropOptimizer```**  . Comparalos con la ejecución anterior del **SGD** utilizando la sigmoide.¿Que optimizador proporciona mejores resultados?¿Es muy alto el impacto del optimizador en la precisión del modelo para esta aplicación? "]},{"cell_type":"code","metadata":{"id":"CokUu-7kQWWq","colab_type":"code","colab":{}},"source":["# Lanzamos la primera ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9b9YQelQXmg","colab_type":"code","colab":{}},"source":["# Lanzamos la segunda ejecución \n","b_size = # ???\n","n_epochs = # ???\n","lr = # ???\n","pkeep_tr = # ???\n","act_function = # ???\n","optimizer = # ???\n","train_deep_net(lr, b_size, n_epochs, act_function, optimizer, pkeep_tr)"],"execution_count":0,"outputs":[]}]}