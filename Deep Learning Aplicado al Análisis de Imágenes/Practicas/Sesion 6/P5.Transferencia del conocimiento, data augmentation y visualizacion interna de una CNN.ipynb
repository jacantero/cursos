{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P5.Transferencia del conocimiento, data augmentation y visualización interna de una CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xPlDWRAKPGRr","colab_type":"text"},"source":["**PRÁCTICA 5. TRANSFERENCIA DEL CONOCIMIENTO, DATA AUGMENTATION Y VISUALIZACIÓN INTERNA DE UNA CNN**"]},{"cell_type":"markdown","metadata":{"id":"pldAhyatPFz_","colab_type":"text"},"source":["**Conceptos necesarios de teoría**:\n","Los de la práctica anterior + transfer learning, fine-tuning, data augmentation y tener claro que son las activaciones y los pesos de los filtros."]},{"cell_type":"markdown","metadata":{"id":"EJTMstoqbn11","colab_type":"text"},"source":["En la sesión anterior nos familizarizamos con las **redes neuronales convolucionales (CNN)** y la librería de desarrollo **Keras**. En esta última práctica vamos a trabajar con **arquitecturas CNN previamente propuestas en la literatura** para la clasificación de imágenes y para el **entrenamiento de nuevos modelos**  a partir de los ya existentes haciendo uso de técnicas las técnicas de **transferencia del conocimiento** y **fine-tuning**. Además comprobaremos como la técnica de aumento sintético de datos (***data augmentation*** del inglés) nos ayuda a mitigar el efecto de overfitting. Por último, aprenderemos los comandos Keras necesarios para **visualizar** tanto las **activaciones** a la salida de ciertas capas de la red como los **pesos de los filtros** de la mismas."]},{"cell_type":"markdown","metadata":{"id":"ArI6tpLdTOLs","colab_type":"text"},"source":["En primer lugar, aprovechando que vamos a necesitar un paquete que no está instalado en el entrono de desarrollo que nos ofrece Colab, vamos a aprender a **listar los paquetes instalados en el entorno de desarrollo** de la máquina asignada por Google y a **instalar nuevos**."]},{"cell_type":"markdown","metadata":{"id":"4Ipr81rvUCqm","colab_type":"text"},"source":["Introduciendo un **símbolo de exclamación (!)** y a continuación una **instrucción del sistema** (i.e. cd, mkdir, rm, etc.) ésta es reconocida sin ningún problema y **ejecutada en la máquina que tenemos por detrás de nuestro Colab**. Por otra parte, tal y como veremos en la sesión teórica \"Hardware/Software necesario para aplicar aprendizaje profundo\" instalando la **herramienta *pip*** en el sistema seré capaz (a través de dicho comando) de **instalar todos los paquetes necesarios en un determinado entorno de desarrollo** (ya veremos este concepto) destinado a técnicas de aprendizaje profundo. ¿Y como puedo entonces observar los **paquetes** que llevo **instalados** (en este caso nos los instaló Google) en mi entorno de desarrollo? Pues **ejecutando** la instrucción de la siguiente celda:"]},{"cell_type":"code","metadata":{"id":"-qUW0oRFcBGd","colab_type":"code","colab":{}},"source":["!pip freeze"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqVIvIm3Xg3-","colab_type":"text"},"source":["¿Habéis visto la cantidad de paquetes que nos prepara Google para que desarrollemos haciendo uso de una gran infinidad de librerias? Hay que decir que de todas esas **librerias** las que a nosotros nos interesan son las destinadas al **aprendizaje profundo (TensorFlow-gpu y Keras)**, las de **procesado de señal e imagen (openCV, pillow, etc.)** y **visualización (matplotlib)**. A pesar de todas las librerías que tenemos instaladas, podemos comprobar que la librería  ***imageio*** no está y vamos a necesitarla. Dicha librería tiene propósitos de lectura y escritura de imágenes ofreciendo compatibilidad con la representación en Google Colab. **Vamos a** proceder a **instalarla** con el comando de la siguiente celda de código:"]},{"cell_type":"code","metadata":{"id":"pNez-TOzb0BJ","colab_type":"code","colab":{}},"source":["# Cuando requiero de un paquete que no está instalado en la máquina que hay por detrás de Colab puedo instalarlo así\n","!pip install imageio"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4HJAadLbZJ6O","colab_type":"text"},"source":["Bueno pues ya sabemos listar e instalar nuevos paquetes en nuestro entorno de desarrollo. Vamos a comenzar con lo que realmente nos interesa en esta práctica, vamos a **cargar una arquitectura de red existente** de las que hemos visto en la sesión teórica y utilizarla para predecir unas cuantas imágenes que vamos a descargar a continuación."]},{"cell_type":"markdown","metadata":{"id":"G4I07o5-vlqQ","colab_type":"text"},"source":["En la práctica anterior vimos como establecer conexión entre nuestro *notebook* de Colab y nuestro google Drive con el objetivo de cargar imágenes en memoria almacenadas en nuestra unidad Drive. En esta sesión vamos a aprender  como **descargar imágenes** (previamente subidas a cierto proveedor de servicios como puede ser https://image.ibb.co) **directamente en la máquina que tenemos detrás de Colab**. Empleando el comando del sistema ***wget*** es posible descargar un contenido en dicha máquina, así que vamos a ello:"]},{"cell_type":"code","metadata":{"id":"e7Cs5tNOvmVY","colab_type":"code","colab":{}},"source":["!wget https://image.ibb.co/cuw6pd/soccer_ball.jpg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBDrsSXrwpuO","colab_type":"text"},"source":["**Listemos** ahora el **contenido del directorio de trabajo** (de la máquina asignada por Google) para comprobar que la imágen se ha almacenado con éxito:"]},{"cell_type":"code","metadata":{"id":"SEg-rgrww1Sj","colab_type":"code","colab":{}},"source":["!ls -la *.jpg*"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5DZV563dvoFj","colab_type":"text"},"source":["Como ya sabéis en el módulo ***keras.applications*** podemos encontrar una gran cantidad de métodos para importar arquitecturas de red previamente propuestas en la literatura. A continuación **vamos a cargar la aquitectura VGG16 y realizar unas cuantas predicciones**, pero antes **abre la documentación de *keras.applications*** haciendo click [aquí](https://keras.io/applications/) y **busca cada una de las funciones** de dicho módulo que se emplean en el código y lee acerca de las mismas."]},{"cell_type":"code","metadata":{"id":"RdiB1sahbW5T","colab_type":"code","colab":{}},"source":["from keras.applications import VGG16\n","from keras.applications import imagenet_utils\n","from keras.preprocessing.image import img_to_array\n","from keras.preprocessing.image import load_img\n","import matplotlib.pyplot as plt\n","import imageio as io\n","import numpy as np\n","import cv2\n","\n","# Cargamos la arquitectura de red del SoA con la que queremos predecir junto a sus pesos \n","print(\"[INFO]: Cargando VGG16...\")\n","input_shape = (224, 224)\n","model = VGG16(weights=\"imagenet\") # ¡¡¡ Búscame en keras.applications !!!\n","\n","# Cargamos la imagen, nos aseguramos de que el tamaño es el adecuado y almacenamos en array\n","image_source = \"soccer_ball.jpg\"\n","image = load_img(image_source, target_size=input_shape)\n","image = np.resize(image, (input_shape[0], input_shape[1], 3))\n","image = img_to_array(image)\n","\n","# Extendemos dimensiones (inputShape[0], inputShape[1], 3) para conseguir (1, inputShape[0], inputShape[1], 3)\n","image = np.expand_dims(image, axis=0)\n","\n","# Pre-procesamos la imagen tal y como fueron pre-procesadas las imágenes que se emplearon para entrenar dicha red\n","image = imagenet_utils.preprocess_input(image) # ¡¡¡ Búscame en keras.applications !!!\n","      \n","# Predecimos la clase de nuestra imagen empleando el modelo\n","print(\"[INFO]: Clasificando imagen con el modelo VGG16\")\n","preds = model.predict(image)\n","# Decodificamos las predicciones\n","P = imagenet_utils.decode_predictions(preds) # ¡¡¡ Búscame en keras.applications !!!\n","\n","# Mostramos las predicciones Rank-5 y su probabilidad\n","for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n","      print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n","\n","# Cargamos la imagen con imageio para poder graficarla sin problemas en Colab\n","# Insertamos un cuadro de texto sobre la imagen indicando la etiqueta con mayor probabilidad y dicho valor\n","img = io.imread(image_source)\n","(imagenetID, label, prob) = P[0][0]\n","cv2.putText(img, \"Label: {}, {:.2f}%\".format(label, prob * 100), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n","plt.imshow(img)\n","plt.axis('off')\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gBjnS2xgBBdC","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mHHgRG0WmlrK","colab_type":"text"},"source":["**EJERCICIO 1.** Ahora que ya sabemos como cargar un modelo en concreto desde el módulo *applications* de Keras, vamos a crear una función que sea capaz de predecir una imagen con una de las siguientes arquitecturas del estado del arte: **VGG16**, **VGG19**, **ResNet50**, **InceptionV3** y **Xception**. La cabecera de la función será la siguiente:\n","\n",">>> ```def predict_image(model_name, image_source)```\n","\n","Ambos parámeros de entrada son **dos cadenas de texto**. La **primera** de ellas hace referencia al modelo que emplearemos para la predicción. Concretamente será la **clave de un diccionario** que deberemos crear. Este diccionario nos permitirá seleccionar la función necesaria para **importar el modelo** de interés empleando Keras. Dicha importación la haremos **descargando los pesos** de la arquitectura entrenados en **ImageNet** (de la misma forma que hemos hecho en el ejemplo anterior). La **segunda cadena** de texto será el **nombre de la imagen** a predecir. \n","\n","**Nota.** La primera vez que carguemos un modelo importando los pesos de ImageNet, éstos se descargarán de forma automática de un repositorio. Dependiendo de la arquitectura, este proceso puede tardar más o menos según el peso de los mismos (500MB si usamos VGGs, unos 100MB en los otros casos)."]},{"cell_type":"code","metadata":{"id":"eq3OVADWlWeq","colab_type":"code","colab":{}},"source":["# Importamos los paquetes necesarios\n","from keras.applications import ResNet50\n","from keras.applications import InceptionV3\n","from keras.applications import Xception \n","from keras.applications import VGG16\n","from keras.applications import VGG19\n","from keras.applications import imagenet_utils\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.preprocessing.image import img_to_array\n","from keras.preprocessing.image import load_img\n","import numpy as np\n","import urllib\n","import cv2\n","import matplotlib.pyplot as plt\n","import imageio as io\n","\n","def predict_image(model_name, image_source):\n","  \n","  # Definimos un diccionario que mapea el nombre de la red con el nombre de la función\n","  # necesaria para importar el modelo y descargar los pesos empleando Keras\n","  MODELS = {\n","    # ???\n","    # ???\n","    # ???\n","    # ???\n","    # ???\n","  }\n","\n","  # Establecemos el tamaño de entrada y la función de preprocesamiento de imagen\n","  # En Keras están definidas las funciones de pre-procesamiento de cada red: \n","  # Tamaño imagen de entrada: VGGs y ResNet (224,224); Inception y Xception (299,299)\n","  # Función para el pre-procesado: VGGs y ResNet --> imagenet_utils.preprocess_input\n","  # Inception y Xception --> inception_v3.preprocess_input\n","  \n","  # En caso que sean VGGs o ResNet\n","  # ???  # Declaro dimensiones\n","  # ???  # Aplico preprocesado\n","  if model_name in (\"inception\", \"xception\"): # Si es Inception o Xception\n","    # ???  # Declaro dimensiones\n","    # ???  # Aplico preprocesado\n","\n","  # Cargamos el modelo empleando la función de Keras obtenida como valor de la clave\n","  # del diccionario que nos pasan como string\n","  print(\"[INFO]: Cargando {}...\".format(model_name))\n","  # ???\n","  # ???\n","\n","  # Cargamos la imagen, nos aseguramos de que el tamaño es el adecuado y almacenamos en array (img_to_array)\n","  print(\"[INFO]: Cargando y pre-procesando la imagen...\")\n","  # ???\n","  # ???\n","  # ???\n","\n","  # Extendemos dimensiones para conseguir (1, inputShape[0], inputShape[1], 3)\n","  # ???\n","\n","  # Pre-procesamos la imagen\n","  # ???\n","\n","  # Predecimos la clase de nuestra imagen y decodificamos\n","  print(\"[INFO] Clasificando la imagen con '{}'...\".format(model_name))\n","  # ???\n","  # ???\n","\n","  # Mostramos las predicciones rank-5 y su probabilidad\n","  for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n","    print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n","\n","  # Cargamos la imagen con imageio para poder graficarla sin problemas en Colab\n","  # Insertamos un cuadro de texto sobre la imagen indicando la etiqueta con mayor probabilidad y este valor\n","  img = io.imread(image_source)\n","  (imagenetID, label, prob) = P[0][0]\n","  cv2.putText(img, \"Label: {}, {:.2f}%\".format(label, prob * 100), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n","  plt.imshow(img)\n","  plt.axis('off')\n","  \n","  # Devolvemos el modelo para posteriormente mostrar información del mismo\n","  # ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUZ82lk_l2Vw","colab_type":"code","colab":{}},"source":["# Descargamos 3 imágenes más\n","!wget https://image.ibb.co/hdoVFJ/bmw.png\n","!wget https://image.ibb.co/h0B6pd/boat.png\n","!wget https://image.ibb.co/eCyVFJ/clint_eastwood.jpg\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PcPIlea_mKNH","colab_type":"code","colab":{}},"source":["# Comprobamos que están en la máquina que nos asigna Colab\n","!ls -la *.*"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcPrZW5GbxM0","colab_type":"text"},"source":["- A continuación comprueba cómo **predicen** las distintas **arquitecturas** las **4 imágenes** descargadas. ¿Qué arquitectura efectúa unas **predicciones más precisas** en líneas generales? Visualiza los esquemas de las distintas arquitecturas de red empleando la instrucción de Keras **model.summary()**."]},{"cell_type":"code","metadata":{"id":"_lRDAtQ_bXpX","colab_type":"code","colab":{}},"source":["# Predicciones para la imagen bwm.png para VGG16\n","# ???\n","# Muestro arquitectura\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0K3i3iQ9j6S","colab_type":"code","colab":{}},"source":["# Predicciones para la imagen bwm.png para VGG19\n","# ???\n","# Muestro arquitectura\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXUr2hUD9jo2","colab_type":"code","colab":{}},"source":["# Predicciones para la imagen bwm.png para INCEPTION\n","# ???\n","# Muestro arquitectura\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgA2E_Bx9jZ8","colab_type":"code","colab":{}},"source":["# Predicciones para la imagen bwm.png para XCEPTION\n","# ???\n","# Muestro arquitectura\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHBfEV8E9ixd","colab_type":"code","colab":{}},"source":["# Predicciones para la imagen bwm.png para RESNET50\n","# ???\n","# Muestro arquitectura\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TTKYbtCqfSwz","colab_type":"text"},"source":["- Por último, vamos a **predecir**, con las distintas arquitecturas del SoA, las **imágenes** (personales o descargadas de Internet) **que almacenamos en Google Drive en la práctica anterior** y que tratamos de predecir con la arquitectura de red que diseñamos para clasificar el dataset CIFAR10."]},{"cell_type":"code","metadata":{"id":"pni6CUtfgQ_W","colab_type":"code","colab":{}},"source":["# Montamos nuestro Google Drive\n","# ???\n","# Selecciono el path de Drive donde tengo la imagen (incluido el nombre de la misma)\n","img_path = # ???\n","# Predecimos la imagen\n","# ???"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNOjyuLqA5Vg","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oSWNxRaXfPDh","colab_type":"text"},"source":["Tal y como hemos visto en la sesión teórica, en muchas de las aplicaciones en las que queremos proponer una solución basada en aprendizaje profundo, los datos disponibles no son suficientes para obtener un buen modelo de predicción entrenando una CNN desde cero. Las técnicas de ***transfer learning*** y ***fine-tuning*** nos permiten **entrenar modelos precisos con conjuntos de datos limitados (i.e. unas 500-1000 muestras por clase)** evitando tener que:\n","\n","- Definir la estructura de la red neuronal\n","\n","- Entrenarla desde cero\n","\n","Dichas técnicas se basan en **emplear arquitecturas CNN predefinidas** y **que fueron entrenadas en** el conjunto de datos **ImageNet** ofreciendo notables resultados: **ResNet, AlexNet, VGG, Inception, DenseNet, etc**.\n","\n","Como ya sabéis, las redes se inicializan con unos pesos aleatorios (normalmente) que tras una serie de épocas consiguen tener unos valores que permiten clasificar adecuadamente nuestras imágenes de entrada.\n","\n","¿Qué pasaría **si pudiésemos inicializar** esos **pesos a unos valores que sabemos que ya son buenos para clasificar un determinado dataset**?\n","\n","De esta forma, **no necesitaríamos** ni un **dataset tan grande** como el necesario si queremos entrenar una red de cero (de cientos de miles o incluso millones de imágenes podríamos pasar a unas pocas miles) **ni** necesitaríamos **esperar un buen número de épocas a que los pesos cogiesen valores buenos para la clasificación**, lo tendrían mucho más fácil debido a su inicialización. Vamos a poner en práctica las dos técnicas más comunes para realizar lo que buscamos y entender las diferencias entre ellas: **Transfer learning** y **Fine-tuning**."]},{"cell_type":"markdown","metadata":{"id":"2iPHa7X5a028","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SpzSILDCk8my","colab_type":"text"},"source":["Como hemos visto en el apartado teórico, **ImageNet** consiste en un dataset de más o menos **1,2 millones de imágenes** para **entrenamiento**, **50.000 para validación** y **100.000 para test**, pertenecientes a **1000 categorías**.\n","\n","Si recordamos el **esquema general de una CNN**, tenemos un **extractor de características** en la primera etapa (**base model**) y después la **etapa de clasificación** (**top model**) tal y como podemos ver en la siguiente figura:\n","\n","![CNN](https://drive.google.com/uc?id=1CApqq7Hmc5gxuzxMQXgrFs8hJ3MJUh4k)\n","\n","Pues bien, la técnica de **transferencia del conocimiento** se basa en **aprovechar los valores de los pesos** (i.e. conocimiento) que adquirieron los filtros involucrados en la CNN cuando fue entrenada en el conjunto de datos **ImageNet**. La idea es dejar \"congelados\" los pesos de estas capas y **añadir un *top model* ad-hoc a nuestra aplicación**, en el que la última capa tendrá tantas neuronas como clases en las que queremos clasificar. **También** podriamos **guardar las características** (i.e. mapa de activación) de la **última capa** (Pooling Later 3 en el caso de la figura) **y entrenar modelos de predicción** basados en otro tipo de **algoritmos de clasificación** (i.e. Random Forests, SVMs, regresión logística, k-NN, etc.).\n","\n","En el siguiente ejemplo vamos a **cargar cierto modelo** del estado del arte (VGG16 en nuestro caso) y vamos a **aprender** cómo **congelar los pesos** y establecer un ***top model* propio**."]},{"cell_type":"code","metadata":{"id":"rq3Vi5mefOXp","colab_type":"code","colab":{}},"source":["# Como siempre los imports que necesitaremos\n","from keras.datasets import cifar10\n","from keras.applications import VGG16\n","from keras.applications import imagenet_utils\n","from tensorflow.keras import optimizers\n","from keras.layers import Dropout, Flatten, Dense\n","import matplotlib.pyplot as plt\n","from keras.engine import Model\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Cargamos el dataset CIFAR10\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","# Si quisieramos pasar a etiquetas en one-hot-encoding (caso de usar categorical_crossentropy en vez de sparse_categorical_crossentropy)\n","#Y_train = to_categorical(y_train)\n","#Y_test = to_categorical(y_test)\n","\n","# Normalizamos las entradas de idéntica forma a como lo hicieron para entrenar la VGG16 en imageNet\n","X_train = imagenet_utils.preprocess_input(X_train)\n","X_test = imagenet_utils.preprocess_input(X_test)\n","# Definimos nuestra lista con el nombre de las clases\n","labelNames = [\"Avión\", \"Automóvil\", \"Pájaro\", \"Gato\", \"Ciervo\", \"Perro\", \"Rana\", \"Caballo\", \"Barco\", \"Camión\"]\n","# Fijamos las dimensiones a las que vamos a re-entrenar el top model. OJO!! con esto \n","# porque si nos traemos también el top_model las dimensiones tendrán que ser identicas\n","# a las de las imágenes con las que se entrenaron la red (no es nuestro caso), lee la documentación en keras.applications.\n","input_shape = (32, 32, 3)\n","\n","# Importamos el modelo con los pesos de imageNet y SIN incluir el top model (i.e. multilayer perceptron para clasificar)\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n","# Vamos a visualizar el modelo prestando especial atención en el número de pesos total y el número de pesos entrenables\n","base_model.summary()\n","\n","# Procedemos a congelar TODAS las capas de nuestro base_model para que no se entrenen\n","# queremos que nuestro feature extractor siga igual que antes => i.e. transfer learning\n","for layer in base_model.layers: \n","  layer.trainable = False\n","  print('Capa ' + layer.name + ' congelada...')\n","\n","# Cogemos la última capa del model y le añadimos nuestro clasificador (top_model)\n","# Fijaros que lo hago utilizando la API funcional, ¿la recordáis?\n","last = base_model.layers[-1].output\n","x = Flatten()(last)\n","x = Dense(1000, activation='relu', name='fc1')(x)\n","x = Dropout(0.3)(x)\n","x = Dense(200, activation='relu', name='fc2')(x)\n","x = Dense(10, activation='softmax', name='predictions')(x)\n","model = Model(base_model.input, x)\n","\n","# Vamos a visualizar el modelo prestando especial atención en el número de pesos total y el número de pesos entrenables, ¿que ocurre?\n","model.summary()\n","\n","# Compilamos el modelo\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","\n","# Lo entrenamos\n","H = model.fit(x=X_train, y=y_train, validation_split=0.2, batch_size=256, epochs=30, verbose=1)\n","\n","# Evaluación del modelo\n","print(\"[INFO]: Evaluando el modelo...\")\n","predictions = model.predict(X_test, batch_size=64)\n","print(classification_report(y_test, predictions.argmax(axis=1), target_names=labelNames))\n","\n","# Gráficas\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, 30), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, 30), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, 30), H.history[\"acc\"], label=\"train_acc\")\n","plt.plot(np.arange(0, 30), H.history[\"val_acc\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcrfKjdygB3R","colab_type":"text"},"source":["- Tras ejecutar el ejempo, **¿que se puede observar?¿es buena la predicción?¿mejor que con la arquitectura simple que propusimos en la práctica anterior?¿se puede decir que hay efecto overfitting?**"]},{"cell_type":"markdown","metadata":{"id":"FeYHta2Lgxsh","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TMgz7s0_gy6F","colab_type":"text"},"source":["Despues de haber visto que la técnica de transferencia del conocimiento, es bastante simple y para problemas complicados no ofrece muy buenos resultados, lo que **vamos a** hacer es **re-entrenar aparte del *top-model* algunos bloques convolucionales del *base_model***. Esta sutil diferencia hace que en vez de estar haciendo transfer learning estemos haciendo **fine-tuning**. Normalmente lo que se suele hacer es ir **re-entrenando bloques convolucionales de atrás hacia delante**, es decir, en una primera ejecución re-entrenaríamos el último bloque convolucional y si vemos gráficamente que el *accuracy* finaliza con una tendencia alcista y sin presentar signos de *overffiting* vamos re-entrenando hacia detrás, i.e. en la siguiente ejecución re-entrenariamos el último bloque convolucional y el penúltimo y así sucesivamente."]},{"cell_type":"markdown","metadata":{"id":"tQ89nIMcodqq","colab_type":"text"},"source":["**EJERCICIO 2.** Para poner en práctica la técnica de fine-tuning, en este ejercicio **vamos a \"descongelar\" los pesos del último bloque convolucional de la VGG16**. De la misma forma que en el ejemplo anterior, **se debe cargar el modelo de la VGG16 junto con los pesos de imageNet** e ir congelando todas las capas hasta llegar al último bloque convolucional que las dejaremos descongeladas o entrenables (así es como se cargan por defecto). Concretamente **la capa a partir de la cual re-entrenaremos** la parte final del base_model y el top_model que incluyamos (el mismo que en el ejemplo anterior) será la ***block5_conv1*** resaltada en la siguiente figura:\n","\n","![texto alternativo](https://drive.google.com/uc?id=1QnkUA4GEhMK5t0-1eu0I-F_gzI7NA2O6)\n","\n","**Nota.** Emplear los **mismos hiper-parámetros** de aprendizaje que en el **ejemplo anterior**. En caso de que en la últimas épocas del proceso de entrenamiento el accuracy bajara drásticamente a 0.1, cambiad la función de pérdidas a categorical_crossentropy con los consiguientes cambios en las etiquetas del dataset (convertir a *one-hot encoding*) y en la llamada al ```classification_report``` (como primer parámetro de entrada espera las etiquetas codificadas según categoría). Notar que dicho decremento brusco se puede producir debido a las inestabilidades numéricas del log (0) en el cálculo de la cross-entropía."]},{"cell_type":"code","metadata":{"id":"Lk2-jsrY52eN","colab_type":"code","colab":{}},"source":["# Imports que vamos a necesitar\n","from keras.datasets import cifar10\n","from keras.applications import VGG16\n","from keras.applications import imagenet_utils\n","from keras.utils import to_categorical\n","from tensorflow.keras import optimizers\n","from keras.layers import Dropout, Flatten, Dense\n","import matplotlib.pyplot as plt\n","from keras.engine import Model\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","#Cargamos el dataset CIFAR10\n","# ???\n","\n","# Normalizamos las entradas de idéntica forma a como lo hicieron para entrenar la VGG16 en imageNet\n","# ???\n","# ???\n","\n","# Definimos dimensiones de nuestros datos de entrada y lista con las categorias de las clases\n","# ???\n","# ???\n","\n","# En caso de inestabilidades numéricas pasar datos a one-hot encoding\n","# ???\n","# ???\n","\n","# Importamos VGG16 con pesos de imagenet y sin top_model especificando tamaño de entrada de datos\n","# ???\n","# Mostramos la arquitectura\n","# ???\n","\n","# Congelamos las capas de los 4 primeros bloques convolucionales, el quinto se re-entrena\n","# En base_model.layers.name tenemos la información del nombre de la capa\n","# ???\n","  # ???\n","    # ???\n","  # ???\n","  print('Capa ' + layer.name + ' congelada...')\n","\n","# Cogemos la última capa del model y le añadimos nuestro clasificador (top_model)\n","# ???\n","# ???\n","# ???\n","# ???\n","# ???\n","# ???\n","# ???\n","\n","# Compilamos el modelo\n","# ???\n","\n","# Vamos a visualizar el modelo prestando especial atención en el número de pesos total y el número de pesos entrenables.\n","# ¿tiene sentido en comparación al ejemplo de transfer learning?\n","# ???\n","\n","# Entrenamos el modelo\n","# ???\n","\n","# Evaluación del modelo\n","print(\"[INFO]: Evaluando el modelo...\")\n","# ???\n","# Obtener el report de clasificación\n","# ???\n","\n","# Gráficas\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, 30), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, 30), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, 30), H.history[\"acc\"], label=\"train_acc\")\n","plt.plot(np.arange(0, 30), H.history[\"val_acc\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuYvq5Bh-sak","colab_type":"text"},"source":["- Tras la ejecución del ejercicio, **¿que se puede decir sobre la precisión del modelo obtenido?** A pesar del claro efecto de *overfitting* que evidencian las gráficas, **¿son buenos los resultados obtenidos para el conjunto de test?¿Son comparables con los obtenidos con la CNN que se propuso en la práctica anterior?**"]},{"cell_type":"markdown","metadata":{"id":"cQS6bLjd-hZw","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"etRGnFdW-eMY","colab_type":"text"},"source":["Una de las técnica más empleadas en soluciones basadas en **CNNs** para **mitigar** el efecto de ***overfitting*** es la **generación sintética de nuevas muestras (o data augmenation)** a partir de diferentes transformaciones geométricas (i.e. giros, traslaciones, rotaciones, escalados, emborronamientos, zooms, etc.). Bien es cierto que hay que andar con cuidado a la hora de aplicar esta técnica ya que las nuevas imágenes generadas sintéticamente toman la clase de la imagen origen, con lo cual, las transformaciones no pueden ser muy exageradas, y mucho menos ocasionar que se parezcan a otra clase.\n","\n","**EJERCICIO 3.** Para aprender cómo poner en práctica esta técnica **empleando la librería Keras** vamos a aplicar **data augmentation** con el objetivo de **reducir el efecto de overfitting** que se producía en el ejercicio anterior. Para ello, deberemos modificar el jercicio anterior atendiendo a los siguientes aspectos:\n","\n","1.   Crear un objeto *ImageDataGenerator* para el entrenamiento (que almacenaremos en una variable que llamaremos **```datagen_train```**) indicando las transformaciones geométricas que queremos realizar y con que parámetros:\n","\n",">>- **```Rotation_range=15, width_shift_range=0.1,  height_shift_range=0.1,  zoom_range=0.1, horizontal_flip=True```**\n",">>- Además debemos especificar ya aquí que nuestro conjunto de training será dividido en una **20% para la validación** del modelo.\n","\n","2. Aplicar el método **```flow```** creando **dos generadores de datos**, uno de **entrenamiento** (almacenarlo en **```train_datagen```**) y otro de **validación** (almacenarlo en **```val_datagen```**). Para generarlos utilizaremos el ```datagen_train``` pasandole los datos y etiquetas de entrenamiento. Pasad como parámetro un **```batch_size=128```** para ambos generadores de datos.\n","\n","4. Realizar el proceso de entrenamiento empleando la función **```fit_generator```** en lugar de ```fit```. Tendremos que pasar como parámetros de entrada el generador con los datos de entrenamiento (**```train_datagen```**), el número de pasos por época en entrenamiento que vendrá dado por la loongitud de los datos de entrenamiento dividido del tamaño de batch, el generador con los datos de validación (**```val_datagen```**) y el número de pasos por época en validación y el número de epocas (**```n_epochs=30```**).\n","\n","**Nota 1.** Es muy importante entender los parámetros de entrada tanto del objeto *ImageDataGenerator* como de lo que hace la función ```flow``` y como funciona ```fit_generator```. Para ello, leed atentamente la documentación de Keras.\n","\n","**Nota 2.** Al realizar el aumento de datos, el proceso de entrenamiento es bastante costoso. Se recomienda almacenar en Google Drive el modelo generado.\n","\n"]},{"cell_type":"code","metadata":{"id":"qibysb6ipm3U","colab_type":"code","colab":{}},"source":["from keras.datasets import cifar10\n","from keras.applications import VGG16\n","from keras.applications import imagenet_utils\n","from keras.utils import to_categorical\n","from tensorflow.keras import optimizers\n","from keras.layers import Dropout, Flatten, Dense\n","import matplotlib.pyplot as plt\n","from keras.engine import Model\n","from sklearn.metrics import classification_report\n","from keras.preprocessing.image import ImageDataGenerator\n","from google.colab import drive\n","from keras.optimizers import Adam\n","import numpy as np\n","\n","#Cargamos el dataset CIFAR10\n","# ???\n","\n","# Normalizamos las entradas de idéntica forma a como lo hicieron para entrenar la VGG16 en imageNet\n","# ???\n","# ???\n","\n","# Definimos dimensiones de nuestros datos de entrada y lista con las categorias de las clases\n","# ???\n","# ???\n","\n","# Importamos VGG16 con pesos de imagenet y sin top_model especificando tamaño de entrada de datos\n","# ???\n","# Mostramos la arquitectura\n","# ???\n","\n","\n","# Data generators para poder hacer data augmentation\n","print('Usando real-time data augmentation.')\n","# ???\n","\n","# Aplicamos comando flow pasando los datos y etiquetas\n","# ???\n","# ???\n","\n","# Congelamos las capas de los 4 primeros bloques convolucionales, el quinto se re-entrena\n","# En base_model.layers.name tenemos la información del nombre de la capa\n","# ???\n","  # ???\n","    # ???\n","  # ???\n","  # ???\n","\n","# Cogemos la última capa del model y le añadimos nuestro clasificador (top_model)\n","# ???\n","# ???\n","# ???\n","# ???\n","# ???\n","# ???\n","# ???\n","\n","# Compilamos el modelo\n","# ???\n","\n","# Vamos a visualizar el modelo prestando especial atención en el número de pesos total y el número de pesos entrenables.\n","# ???\n","\n","# Calculemos el parámetro steps_per_epoch para establecer el orden de magnitud de nuestro data augmentation\n","# Se debe tener en cuenta que estamos empleando el 80% del training set para entrenar y el 20% para validar\n","# Vamos a asumir que queremos crear dos muestras por cada una que tenemos en el training_set \n","# ???\n","\n","# Entrenamos el modelo\n","# ???\n","\n","# Almaceno el modelo en Drive\n","# Montamos la unidad de Drive\n","# ???\n","# Almacenamos el modelo empleando la función mdoel.save de Keras\n","# ???\n","\n","# Evaluación del modelo\n","print(\"[INFO]: Evaluando el modelo...\")\n","# Efectuamos predicciones\n","# ???\n","# Evaluamos\n","# ???\n","\n","# Gráficas\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, 30), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, 30), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, 30), H.history[\"acc\"], label=\"train_acc\")\n","plt.plot(np.arange(0, 30), H.history[\"val_acc\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bvyvJh5cMn4Q","colab_type":"text"},"source":["- Tras la ejecución del ejercicio, **¿que se puede decir sobre la precisión del modelo obtenido?**, **¿se aprecian ahora signos de overfitting?**, **¿son buenos los resultados obtenidos para el conjunto de test?, ¿son comparables con los obtenidos con la CNN que se propuso en la práctica anterior?**"]},{"cell_type":"markdown","metadata":{"id":"BdC5lWJwwhLC","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SZCivOAkNHRK","colab_type":"text"},"source":["Para finalizar con la última práctica del curso vamos a intentar entender un poco más en profundidad que ocurre dentro de una CNN. Para conseguirlo debemos saber que existen dos cosas fundamentales que podemos visualizar:\n","\n","-  **Los mapas de activaciones a la salida de las capas.** Son simplemente los resultados que obtenemos a la salida de una determinada capa durante el *forward pass*. Normalmente, cuando visualizamos las activaciones de una red con activaciones de tipo ReLU, necesitamos unas cuantas épocas antes de empezar a ver algo útil. Una cosa para la que son muy útiles es para ver si algún filtro está completamente negro para diferentes entradas, es decir, todos sus elementos son siempre 0. Esto significa que el filtro está muerto, y normalmente pasa cuando entrenamos con learning rates altos.\n","\n","- **Los filtros aprendidos de los bloques convolucionales**. Normalmente, estos filtros son más interpetables en las primeras capas de la red que en las últimas. Sobre todo, es útil visualizar los filtros de la primera, que está mirando directamente a las imágenes de entrada. Una red bien entrenada tendrá filtros perfectamente definidos, al menos en las primeras capas, y sin practicamente ruido. Si por el contrario tuviésemos filtros con mucho ruido podría deberse a que hace falta entrenar más la red, o a que tenemos overfitting y necesitamos algún método de regularización.\n"]},{"cell_type":"markdown","metadata":{"id":"dSQLujE6PLE5","colab_type":"text"},"source":["A continuación vamos a llevar a cabo un ejemplo para poner todo lo anterior en práctica. **En primer lugar**, vamos a ver cómo se pueden **visualizar las activaciones de la última capa de nuestra CNN** (llamadas ***saliency map***). Para ello, necesitamos **cambiar la activación de la última capa**, de softmax **a lineal**, para una correcta visualización y antes que nada debemos instalar una librería que nos permita visualizar el interior de las CNNs denominada ***keras-vis***."]},{"cell_type":"code","metadata":{"id":"dUSuz8p-yehF","colab_type":"code","colab":{}},"source":["#!pip install keras-vis\n","#!pip install git+https://github.com/raghakot/keras-vis.git@137f268b77f40a12d373db17da74dcc9fe759680\n","!pip install git+https://github.com/raghakot/keras-vis.git -U\n","from keras.applications import VGG16\n","from vis.utils import utils\n","from keras import activations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7lb7CdtydQ9","colab_type":"code","colab":{}},"source":["# Importamos la VGG16 con los pesos de ImageNet y su top_model original\n","model = VGG16(weights='imagenet', include_top=True)\n","\n","# Compilamos el modelo\n","model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Buscamos la capa que estamos interesados en visualizar empleando el método find_layer_idx\n","layer_idx = utils.find_layer_idx(model, 'predictions')\n","\n","# Cambiamos la activación softmax por la función de activación lineal y aplicamos modificaciones\n","model.layers[layer_idx].activation = activations.linear\n","model = utils.apply_modifications(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e04qY-L1QpMj","colab_type":"text"},"source":["Acto seguido **cargamos un par de imágenes** sobre las que vamos a ver los mapas de activaciones."]},{"cell_type":"code","metadata":{"id":"cj3_n1cgFw78","colab_type":"code","colab":{}},"source":["# Imports necesarios\n","from vis.utils import utils\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (18, 6) # tamaño de las imágenes\n","\n","# Cargamos dos imágenes\n","img1 = utils.load_img('https://image.ibb.co/ma90yJ/ouzel2.jpg', target_size=(224, 224))\n","img2 = utils.load_img('https://image.ibb.co/djhyky/ouzel1.jpg', target_size=(224, 224))\n","\n","# Las mostramos\n","f, ax = plt.subplots(1, 2)\n","ax[0].imshow(img1)\n","ax[0].grid(False)\n","ax[1].imshow(img2)\n","ax[1].grid(False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"upGEMjZQQ7vy","colab_type":"text"},"source":["La función que se encarga de mostrarnos el mapa de activación es **```visualize_saliency```** perteneciente al módulo de visualización de la librería ***keras-vis***. A dicha función tenemos que **pasarle el modelo, el ID de la capa, el ID de la clase para la que queremos ver las activaciones, y la imagen para la que queremos ver las activaciones**.\n","\n","¿Y qué es eso del **ID de la clase** para la que queremos ver las activaciones? Pues ese ID es un **identificador único que tiene cada una de las 1000 clases del dataset ImageNet**. La clase pájaro es la ID=20, por lo cual, si introducimos una imagen de un pájaro, debería activarse bastante dicha clase, e indicarnos en qué se fija para decidir que efectivamente es un pájaro. Si para la misma imagen de entrada (un pájaro) utilizamos ID=64, la red buscaría una *green mamba*, que es una serpiente (como la de la figura de la derecha) por lo que las activaciones deberían ser mucho menores. Vamos a **visualizar** las **activaciones** de la **neurona ID=20** para nuestras imágenes de entrada.\n","\n","![Paj_serp](https://drive.google.com/uc?id=1RcJ2tFw4_lLtwTHK8xUl0QpsMTK1-MF2)\n","\n","Para conocer el listado completo de las 1000 clases de ImageNet con sus correspondiente IDs haced click en el siguiente enlace: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"]},{"cell_type":"code","metadata":{"id":"uj0tSny1GT-6","colab_type":"code","colab":{}},"source":["from vis.visualization import visualize_saliency, overlay\n","from vis.utils import utils\n","from keras import activations\n","\n","# Con esta línea encontramos el índice de la capa predicciones, que es la que queremos ver sus activaciones\n","layer_idx = utils.find_layer_idx(model, 'predictions')\n","print('Número de capa: ', layer_idx)\n","\n","f, ax = plt.subplots(1, 2)\n","for i, img in enumerate([img1, img2]):    \n","    \n","    # 20: 'water ouzel, dipper'\n","    grads = visualize_saliency(model, layer_idx, filter_indices=20, seed_input=img, backprop_modifier='guided')\n","    \n","    # Vamos a ver las activaciones con el colormap=jet, que es adecuado para ver mapas de probabilidades\n","    ax[i].imshow(grads, cmap='jet')\n","    ax[i].grid(False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KtbVVrEsWFCh","colab_type":"text"},"source":[" A continuación, vamos a probar **otro método de visualización**: el ***cam-saliency***. En este caso, la visualización contiene **más detalles**, ya que **hace uso de la información** no solo de la capa indicada, sino **de la anterior capa Conv o Pool** que encuentre. En nuestro caso sera la capa *max_pool5*."]},{"cell_type":"code","metadata":{"id":"k4vtY87oHEak","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.cm as cm\n","from vis.visualization import visualize_cam, overlay\n","\n","plt.figure()\n","f, ax = plt.subplots(1, 2)\n","for i, img in enumerate([img1, img2]):    \n","  \n","  # Como no le hemos indicado el parámetro penultimate_layer_idx, escoge la primera que encuentra, que es la max_pool5\n","  grads = visualize_cam(model, layer_idx, filter_indices=20, seed_input=img, backprop_modifier='guided')        \n","        \n","  # Sobreponemos el mapa de activaciones en la imagen original\n","  jet_heatmap = np.uint8(cm.jet(grads) * 255)[:, :, 0:3]\n","  ax[i].imshow(overlay(jet_heatmap, img))\n","  ax[i].grid(False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KYyEqt3cVYm","colab_type":"text"},"source":["¿Qué os ha parecido esto?¿En qué **partes del pájaro** está prestando **atención** la **CNN** para tomar una decisión y **clasificar**?"]},{"cell_type":"markdown","metadata":{"id":"F3-YSJc0XVt-","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DvRqGkLOwhkl","colab_type":"text"},"source":["Una vez vistos los mapas de activación, vamos a proceder a ver distintos **filtros de diferentes capas convolucionales**. En primer lugar, vamos a visualizar los filtros de la **primera capa convolucional**:\n","\n"]},{"cell_type":"code","metadata":{"id":"oZL4NalDIMnG","colab_type":"code","colab":{}},"source":["from vis.visualization import visualize_activation, get_num_filters\n","\n","# Buscamos el índice de la capa cuyos filtros queremos visualizar\n","layer_name = 'block1_conv2'\n","layer_idx = utils.find_layer_idx(model, layer_name)\n","\n","# Creamos un array con valores de 0 al núm de filtros a visualizar (en nuestro caso 4 por motivos temporales)\n","filters = np.arange(4)\n","\n","# Guardamos cada filtro en vis_images\n","plt.rcParams['figure.figsize'] = (18, 6) # Tamaño de las imágenes de los filtros\n","vis_images = []\n","for idx in filters:\n","    img = visualize_activation(model, layer_idx, filter_indices=idx)\n","    \n","    # Escribimos el índice del filtro\n","    img = utils.draw_text(img, 'Filter {}'.format(idx))    \n","    vis_images.append(img)\n","\n","# Generamos una imagen donde se visualizen todos\n","stitched = utils.stitch_images(vis_images, cols=8)    \n","plt.axis('off')\n","plt.imshow(stitched)\n","plt.title(layer_name)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5GGaqcSYAdn","colab_type":"text"},"source":["Para finalizar, vamos a **visualizar filtros** de diferentes capas pertenecientes a **distintos bloques convolucionales**:"]},{"cell_type":"code","metadata":{"id":"Rb17iltqIcHG","colab_type":"code","colab":{}},"source":["selected_indices = []\n","for layer_name in ['block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']:\n","    layer_idx = utils.find_layer_idx(model, layer_name)\n","\n","    # seleccionamos aleatoriamente 4 filtros de cada capa\n","    filters = np.random.permutation(get_num_filters(model.layers[layer_idx]))[:4]\n","    selected_indices.append(filters)\n","\n","    # generamos el mapa de activaciones\n","    vis_images = []\n","    for idx in filters:\n","        img = visualize_activation(model, layer_idx, filter_indices=idx)\n","\n","        # escribimos el número de filtro\n","        img = utils.draw_text(img, 'Filter {}'.format(idx))    \n","        vis_images.append(img)\n","\n","    # generamos la imagen final a visualizar\n","    stitched = utils.stitch_images(vis_images, cols=4)\n","    plt.figure()\n","    plt.axis('off')\n","    plt.imshow(stitched)\n","    plt.show()"],"execution_count":0,"outputs":[]}]}